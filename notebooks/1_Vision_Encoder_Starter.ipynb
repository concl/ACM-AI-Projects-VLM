{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X42ej8ZhF1q4"
      },
      "source": [
        "## Goals\n",
        "The goals of this assignment are to:\n",
        "\n",
        "1. Build a Vision Transformer (ViT) from scratch .\n",
        "2. Build a Semantic Segmentation model using a ViT encoder\n",
        "\n",
        "By the end of this assignment, you will have gained experience with:\n",
        "\n",
        "- Working with PyTorch and the MiniPlaces dataset for image classification.\n",
        "- Implementing and training different types of neural networks using PyTorch.\n",
        "- Debugging and troubleshooting issues that may arise during the development process.\n",
        "\n",
        "\n",
        "Good luck and happy coding! Remember, the most important thing is to have fun and learn something new.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L6ZjCc5F1q5"
      },
      "source": [
        "## Setup Code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKJx4TLuF1q5"
      },
      "source": [
        "To begin, you will need to download the MiniPlaces dataset using the provided link.\n",
        "\n",
        "-----\n",
        "\n",
        "\n",
        "Recall the introduction about the storage system of CoLab we went through in the assignment 1. For efficient development of our models, we will still use the temporary storage space to hold our data. This means that every time you open up this notebook, we will need to re-download and process the dataset. Don't worry though - this shouldn't take long, usually just a minute or less. Okay, let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNtgNN3FF1q6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab762892-1c72-4039-8430-5eeb95acce3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n",
            "--2024-03-02 20:30:30--  https://web.cs.ucla.edu/~smo3/data.tar.gz\n",
            "Resolving web.cs.ucla.edu (web.cs.ucla.edu)... 131.179.128.29\n",
            "Connecting to web.cs.ucla.edu (web.cs.ucla.edu)|131.179.128.29|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 460347416 (439M) [application/x-gzip]\n",
            "Saving to: ‘data.tar.gz’\n",
            "\n",
            "data.tar.gz         100%[===================>] 439.02M  16.2MB/s    in 26s     \n",
            "\n",
            "2024-03-02 20:30:57 (17.0 MB/s) - ‘data.tar.gz’ saved [460347416/460347416]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "# Downloading this file takes about a few seconds.\n",
        "# Download the tar.gz file from google drive using its file ID.\n",
        "!pip3 install --upgrade gdown --quiet\n",
        "!wget https://web.cs.ucla.edu/~smo3/data.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB8tvZhIF1q9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "import urllib.request\n",
        "\n",
        "def setup(file_link_dict={},\n",
        "          folder_name='Assignment3'):\n",
        "  # Let's make our assignment directory\n",
        "  CS188_path = './'\n",
        "  os.makedirs(os.path.join(CS188_path, 'Assignment3', 'data'), exist_ok=True)\n",
        "  # Now, let's specify the assignment path we will be working with as the root.\n",
        "  root_dir = os.path.join(CS188_path, 'Assignment3')\n",
        "  # Open the tar.gz file\n",
        "  tar = tarfile.open(\"data.tar.gz\", \"r:gz\")\n",
        "  # Extract the file \"./Assignment3/data\" folder\n",
        "  total_size = sum(f.size for f in tar.getmembers())\n",
        "  with tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Extracting tar.gz file\") as pbar:\n",
        "      for member in tar.getmembers():\n",
        "          tar.extract(member, os.path.join(root_dir, 'data'))\n",
        "          pbar.update(member.size)\n",
        "  # Close the tar.gz file\n",
        "  tar.close()\n",
        "  # Next, we download the train/val/test txt files:\n",
        "  for file_name, file_link in file_link_dict.items():\n",
        "      print(f'Downloding {file_name}.txt from {file_link}')\n",
        "      urllib.request.urlretrieve(file_link, f'{root_dir}/data/{file_name}.txt')\n",
        "  return root_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx-2pvciF1q-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c1211d-1a6e-4f2f-b121-0799217e9561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting tar.gz file: 100%|██████████| 566M/566M [00:33<00:00, 16.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloding train.txt from https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/train.txt\n",
            "Downloding val.txt from https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/val.txt\n"
          ]
        }
      ],
      "source": [
        "\n",
        "val_url = 'https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/val.txt'\n",
        "train_url = 'https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/train.txt'\n",
        "root_dir = setup(\n",
        "    file_link_dict={'train':train_url, 'val':val_url},\n",
        "    folder_name='Assignment3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4MyTph9F1q_"
      },
      "source": [
        "### Define the data transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v9w2JugF1rA"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "image_net_mean = torch.Tensor([0.485, 0.456, 0.406])\n",
        "image_net_std = torch.Tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "# Notice we are resize images to 128x128 instead of 64x64.\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize([128, 128]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(image_net_mean, image_net_std),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXrmPXGEF1rB"
      },
      "source": [
        "### Define the dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoHHge5YF1rB"
      },
      "outputs": [],
      "source": [
        "# You can copy your dataset from Assignment2.\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class MiniPlaces(Dataset):\n",
        "    def __init__(self, root_dir, split, transform=None, label_dict=None):\n",
        "        \"\"\"\n",
        "        Initialize the MiniPlaces dataset with the root directory for the images,\n",
        "        the split (train/val/test), an optional data transformation,\n",
        "        and an optional label dictionary.\n",
        "\n",
        "        Args:\n",
        "            root_dir (str): Root directory for the MiniPlaces images.\n",
        "            split (str): Split to use ('train', 'val', or 'test').\n",
        "            transform (callable, optional): Optional data transformation to apply to the images.\n",
        "            label_dict (dict, optional): Optional dictionary mapping integer labels to class names.\n",
        "        \"\"\"\n",
        "        assert split in ['train', 'val', 'test']\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.filenames = []\n",
        "        self.labels = []\n",
        "\n",
        "        self.label_dict = label_dict if label_dict is not None else {}\n",
        "\n",
        "        with open(f'{root_dir}/{split}.txt', 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            image_path, label = line.split(' ')\n",
        "            label = int(label)\n",
        "            self.filenames.append(image_path)\n",
        "            self.labels.append(label)\n",
        "            if split == 'train':\n",
        "                self.label_dict[label] = image_path.split('/')[-2]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the number of images in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of images in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Return a single image and its corresponding label when given an index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the image to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tuple containing the image and its label.\n",
        "        \"\"\"\n",
        "        label = self.labels[idx]\n",
        "        image_path = self.filenames[idx]\n",
        "        image = Image.open(os.path.join(self.root_dir, f'images/{image_path}'))\n",
        "        image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_th2HTIF1rC"
      },
      "source": [
        "### Define the train method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfX-uwDDF1rC"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
        "    \"\"\"\n",
        "    Train the MLP classifier on the training set and evaluate it on the validation set every epoch.\n",
        "\n",
        "    Args:\n",
        "        model (MLP): MLP classifier to train.\n",
        "        train_loader (torch.utils.data.DataLoader): Data loader for the training set.\n",
        "        val_loader (torch.utils.data.DataLoader): Data loader for the validation set.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer to use for training.\n",
        "        criterion (callable): Loss function to use for training.\n",
        "        device (torch.device): Device to use for training.\n",
        "        num_epochs (int): Number of epochs to train the model.\n",
        "    \"\"\"\n",
        "    # Place model on device\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        # Use tqdm to display a progress bar during training\n",
        "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}') as pbar:\n",
        "            for inputs, labels in train_loader:\n",
        "                # Move inputs and labels to device\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero out gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Compute the logits and loss\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                # Backpropagate the loss\n",
        "                loss.backward()\n",
        "\n",
        "                # Update the weights\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update the progress bar\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        avg_loss, accuracy = evaluate(model, val_loader, criterion, device)\n",
        "        print(f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}')\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate the MLP classifier on the test set.\n",
        "\n",
        "    Args:\n",
        "        model (MLP): MLP classifier to evaluate.\n",
        "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
        "        criterion (callable): Loss function to use for evaluation.\n",
        "        device (torch.device): Device to use for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        float: Average loss on the test set.\n",
        "        float: Accuracy on the test set.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0.0\n",
        "        num_correct = 0\n",
        "        num_samples = 0\n",
        "\n",
        "        for inputs, labels in test_loader:\n",
        "            # Move inputs and labels to device\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Compute the logits and loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute the accuracy\n",
        "            _, predictions = torch.max(logits, dim=1)\n",
        "            num_correct += (predictions == labels).sum().item()\n",
        "            num_samples += len(inputs)\n",
        "\n",
        "    # Compute the average loss and accuracy\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = num_correct / num_samples\n",
        "\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2Z9TE_UF1rF"
      },
      "outputs": [],
      "source": [
        "def compute_distances_no_loops(x_train, x_test):\n",
        "  num_train = x_train.shape[0]\n",
        "  num_test = x_test.shape[0]\n",
        "  dists = x_train.new_zeros(num_train, num_test)\n",
        "\n",
        "  A = x_train.reshape(num_train,-1)\n",
        "  B = x_test.reshape(num_test,-1)\n",
        "  AB2 = A.mm(B.T)*2\n",
        "  dists = ((A**2).sum(dim = 1).reshape(-1,1) - AB2 + (B**2).sum(dim = 1).reshape(1,-1))**(1/2)\n",
        "  return dists\n",
        "\n",
        "def predict_labels(dists, y_train, k=1):\n",
        "  num_train, num_test = dists.shape\n",
        "  y_pred = torch.zeros(num_test, dtype=torch.int64)\n",
        "\n",
        "  values, indices = torch.topk(dists, k, dim=0, largest=False)\n",
        "  for i in range(indices.shape[1]):\n",
        "    _, idx = torch.max(y_train[indices[:,i]].bincount(), dim = 0)\n",
        "    y_pred[i] = idx\n",
        "  return indices, y_pred\n",
        "\n",
        "class KnnClassifier:\n",
        "  def __init__(self, x_train, y_train):\n",
        "    self.x_train = x_train\n",
        "    self.y_train = y_train\n",
        "\n",
        "  def predict(self, x_test, k=1):\n",
        "    y_test_pred = None\n",
        "\n",
        "    dists = compute_distances_no_loops(self.x_train, x_test)\n",
        "    _, y_test_pred =  predict_labels(dists, self.y_train, k)\n",
        "\n",
        "    return y_test_pred\n",
        "\n",
        "  def check_accuracy(self, x_test, y_test, k=1, quiet=False):\n",
        "    y_test_pred = self.predict(x_test, k=k)\n",
        "    num_samples = x_test.shape[0]\n",
        "    num_correct = (y_test == y_test_pred).sum().item()\n",
        "    accuracy = 100.0 * num_correct / num_samples\n",
        "    msg = (f'Got {num_correct} / {num_samples} correct; '\n",
        "           f'accuracy is {accuracy:.2f}%')\n",
        "    if not quiet:\n",
        "      print(msg)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pob9aqlkF1rF"
      },
      "outputs": [],
      "source": [
        "# Also, seed everything for reproducibility\n",
        "# code from https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964#file-seed_everything-py\n",
        "def seed_everything(seed: int):\n",
        "    import random, os\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMTQ7y7lF1rF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b592c2-403a-4cc4-c252-6d4526dd1942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda. Good to go!\n"
          ]
        }
      ],
      "source": [
        "# Define the device to use for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device == torch.device('cuda'):\n",
        "    print(f'Using device: {device}. Good to go!')\n",
        "else:\n",
        "    print('Please set GPU via Edit -> Notebook Settings.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y67XuOdxF1rG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834fc971-46b6-4e2c-cd57-2f5b9e0cce0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar  2 20:31:45 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8              10W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "532GJGCHF1rI"
      },
      "source": [
        "## Q1: Steps to build a ViT from scratch\n",
        "Vision Transformer (ViT) is a state-of-the-art neural network architecture for image classification tasks. Unlike traditional convolutional neural networks (CNNs), which have been the standard in computer vision for many years, ViT relies on a self-attention mechanism to extract features from images. This approach has shown to achieve competitive results on various benchmark datasets, while also offering the flexibility to handle tasks that require attention over long-range dependencies in images. ViT has quickly gained popularity in the computer vision community, and has spurred further research into the use of self-attention mechanisms in other areas of deep learning.\n",
        "\n",
        "You will implement the ViT model on the Miniplaces dataset.\n",
        "\n",
        "To implement ViT model for image classification, you will need to follow these steps：\n",
        "1.  Extract feature vectors from the input images using a trainable linear projection layer, which converts the 2D image patches into 1D feature vectors.\n",
        "2. Positional encoding: Add a learnable positional encoding to each feature vector, which provides spatial information to the model.\n",
        "3. Transformer encoder: Stack multiple Transformer encoder layers to process the encoded features, which allows the model to learn both local and global interactions between the image patches.\n",
        "4. Classification head: Add a classification head on top of the final encoded feature vector, which maps the learned representations to the corresponding class labels.\n",
        "5. Training and evaluation: Train the ViT model using an appropriate optimization algorithm and loss function, and evaluate its performance on the validation and testing sets.\n",
        "\n",
        "If you are not familiair with ViT model, then you can read our textbook [Transformers for Vision](https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#fig-vit), or review our [discussion slides](https://drive.google.com/file/d/1RKSnE9MOAGBu9T-_2TaBEm4ASF189Fms/view)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3WejhKIF1rI"
      },
      "source": [
        "### Q1.1: Tokenization:\n",
        "At this step, we need to divide each image into a set of non-overlapping patches, and treat each patch as a token. This is the key step that distinguishes ViT from other computer vision models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yktdHkZrKPVw"
      },
      "source": [
        "#### Q1.1.1 Tokenize_image Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDPametLF1rJ"
      },
      "outputs": [],
      "source": [
        "def tokenize_image(img, patch_size=16, stride=16):\n",
        "    \"\"\"\n",
        "    Tokenize an image into non-overlapping image patches.\n",
        "    Args:\n",
        "        img (torch.Tensor): The input image with shape (C, H, W).\n",
        "        patch_size (int): The size of each patch.\n",
        "        stride (int): The stride of the sliding window.\n",
        "    Returns:\n",
        "        patches (torch.Tensor): The tokenized patches with shape (N, patch_size*patch_size*C).\n",
        "    \"\"\"\n",
        "\n",
        "    C, H, W = img.shape\n",
        "    patches = []\n",
        "    # Each patch is flattened into a 1-dimensional vector and stacked into a\n",
        "    # tensor with shape (N, patch_size(H) * patch_size(W) * C), where N is the number of patches.\n",
        "    # We only consider the case image size can be modulo by the patch_size\n",
        "\n",
        "    # Additionally, before flattening, remember to permute the patch such that\n",
        "    # it has shape (patch_size(H), patch_size(W), C)\n",
        "    for i in range(0,H,stride):\n",
        "        for j in range(0,W,stride):\n",
        "            flat = img[:,i:i+patch_size,j:j+patch_size]\n",
        "            flat = flat.permute(1, 2, 0)\n",
        "            flat = torch.flatten(flat)\n",
        "            patches.append(flat)\n",
        "    patches = torch.stack(patches)\n",
        "    return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVfIbK5tF1rJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3344e7ae-b354-45aa-e44e-be2c839256e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good! For patch_size: 32, the output match\n",
            "Good! For patch_size: 16, the output match\n",
            "Good! For patch_size: 8, the output match\n",
            "Good! For patch_size: 4, the output match\n",
            "Good! For patch_size: 2, the output match\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# test your implementation of tokenize_image\n",
        "random_img = torch.rand(3,64,64)\n",
        "patched_img = tokenize_image(random_img,8,8)\n",
        "\n",
        "for i in [32,16,8,4,2]:\n",
        "    out = tokenize_image(random_img,i,i)\n",
        "\n",
        "    fast_patch = Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = i, p2 = i)\n",
        "\n",
        "    answer = fast_patch(random_img.unsqueeze(0))\n",
        "    equal = torch.allclose(out,answer.squeeze(0))\n",
        "    #print('Difference: ', equal)\n",
        "    if equal:\n",
        "      print('Good! For patch_size: %d, the output match' %(i))\n",
        "    else:\n",
        "      print('Uh-oh! For patch_size: %d, the output are different' %(i))\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZo2ygYUKfWC"
      },
      "source": [
        "#### Q1.1.2 linear projection layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQcWs0XTIC7P"
      },
      "source": [
        "At this step, you will need to implement the linear projection linear project layer combined with tokenize operation.\n",
        "\n",
        "This layer is used to transfer a single image to the image embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ9tqkaPK1g7"
      },
      "outputs": [],
      "source": [
        "class Tokenization_layer(nn.Module):\n",
        "  def __init__(self, dim, patch_dim,patch_height, patch_width):\n",
        "    super().__init__()\n",
        "    \"\"\"\n",
        "        Args:\n",
        "          dim (int): input and output dimension.\n",
        "          patch_dim(int): falttened vectot dimension for image patch\n",
        "          patch_height (int): height of one image patch\n",
        "          patch_weight (int): weight of one image patch\n",
        "\n",
        "        You can use Pytorch's built-in function and the above Rearrange method.\n",
        "        Input and output shapes of each layer:\n",
        "        1) Rerrange the image: (batch_size, channels, H,W) -> (batch_size,N,patch_dim)\n",
        "        2) Norm Layer1 (LayerNorm): (batch_size,N,patch_dim) -> (batch_size,N,patch_dim)\n",
        "        3) Linear Projection layer: (batch_size,N,patch_dim) -> (batch_size,N,dim)\n",
        "        4) Norm Layer2 (LayerNorm): (batch_size,N,dim) -> (batch_size,N,dim)\n",
        "    \"\"\"\n",
        "\n",
        "    self.to_patch = Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)\n",
        "    self.norm1 = None\n",
        "    self.fc1 = None\n",
        "    self.norm2 = None\n",
        "\n",
        "    ################# Your Implementations #################################\n",
        "    # Hints: You can use the Rearrange method above to achieve faster patch operation\n",
        "    # Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)\n",
        "\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x (torch.Tensor): input tensor in the shape of (batch_size,C,H,W)\n",
        "    Return:\n",
        "      out (torch.Tensor): output patch embedding tensor in the shape of (batch_size,N,dim)\n",
        "\n",
        "     The input tensor 'x' should pass through the following layers:\n",
        "     1) self.to_patch: Rerrange image\n",
        "     2) self.norm1: LayerNorm\n",
        "     3) self.fc1: Fully-Connected layer\n",
        "     4) self.norm2: LayerNorm\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    out = None\n",
        "    ################# Your Implementations #################################\n",
        "\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9b7ESVsF1rK"
      },
      "source": [
        "### Q1.2 Attention:\n",
        "You will need to follow the steps to implement multi-head attention in this question.\n",
        "1. **Obtain Q,K,V vectors**: To obtain the Q, K, and V vectors, the input vectors are processed through three distinct single linear layers. In our implementation, we use a single linear layer with 3xD output channels, and then we divide the output into three chunks. We consider the first chunk as the Q vectors, the second chunk as the K vectors, and the last chunk as the V vectors.\n",
        "\n",
        "2. **Calculate similarity**: Compute the similarity scores between query vectors and a set of key vectors using a dot product.\n",
        "\n",
        "3. **Apply softmax**: Apply a softmax function to normalize the similarity scores across the key vectors. This creates a probability distribution that represents the relative importance of each key vector with respect to the query vector.\n",
        "\n",
        "4. **Compute weighted sum**: Compute a weighted sum of the** value vectors**, where the weights are the probability distribution obtained in step 2. This produces a context vector that summarizes the most relevant information from the value vectors with respect to the query vector.\n",
        "\n",
        "5. **Concatenate output**: The outputs of each head are then concatenated and passed through another linear projection to produce the final output.\n",
        "\n",
        "For more details, you can read our [textbook](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7er82fMhF1rK"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          dim (int): input and output dimension.\n",
        "          heads (int): number of attention heads.\n",
        "          dim_head (int): input dimension of each attention head.\n",
        "          dropout (float): dropout rate for attention and final_linear layer.\n",
        "\n",
        "        Initialize a attention block.\n",
        "        You can use Pytorch's built-in function.\n",
        "        Input and output shapes of each layer:\n",
        "        1) Define the inner dimension as number of heads* dimension of each head\n",
        "        2) to_qkv: (batch_size, dim) -> (batch_size,3*inner_dimension)\n",
        "        3) final_linear: (batch_size, inner_dim) -> (batch_size, dim)\n",
        "        \"\"\"\n",
        "\n",
        "        self.heads = heads\n",
        "        self.dim_head = dim_head\n",
        "\n",
        "        self.inner_dim = dim_head *  heads\n",
        "\n",
        "\n",
        "        self.attend = None\n",
        "        self.dropout = None\n",
        "        self.final_linear = None\n",
        "\n",
        "\n",
        "        # Here, you should define\n",
        "        # 1) self.to_qkv: (batch_size, dim) -> (batch_size,3*inner_dimension)\n",
        "        # 2) self.dropout: Dropout layer with ratio defined by dropout variable\n",
        "        # 3) self.final_linear: (batch_size, inner_dim) -> (batch_size, dim)\n",
        "        ################# Your Implementations #################################\n",
        "\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Forward pass of the attention block.\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor in the shape of (batch_size,N,dim).\n",
        "        Returns:\n",
        "            out (torch.Tensor): output tensor in the shape of (batch_size,N,dim).\n",
        "\n",
        "        The input tensor 'x' should pass through the following layers:\n",
        "        1) to_qkv: (batch_size,N,dim) -> (batch_size,N,3*inner_dimension)\n",
        "        2) Divide the ouput of to qkv to q,k,v and then divide them in to n heads\n",
        "            (batch_size,N,inner_dim) -> (batch_size,N,num_head,head_dim)\n",
        "        3) Use torch.matmul to get the product of q and k\n",
        "        4) Divide the above tensor by the squre root of head dimension\n",
        "        5) Apply softmax and then dropout on the above tensor\n",
        "        6) Mutiply the above tensor with v to get attention\n",
        "        7) Concatenate the attentions from multi-heads\n",
        "            (batch_size,N,num_head,head_dim) -> (batch_size,N,inner_dim)\n",
        "        8) Pass the output from last step to a fully connected layer\n",
        "        9) Apply dropout for the last step output\n",
        "        '''\n",
        "        out = None\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "        ################# Your Implementations #################################\n",
        "        # Hint you can use :\n",
        "        #    out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        # to concatenate the output from all attention heads\n",
        "        # This operation will change the tensor shape from (batch_size,N,num_head,head_dim)\n",
        "        # to  (batch_size,N,inner_dim)\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7yya6zjF1rL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80b58c5b-fe87-4626-9756-8b4e7b80d800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good! For input dim: 512, the output shape is correct\n",
            "Good! For input dim: 768, the output shape is correct\n",
            "Good! For input dim: 1096, the output shape is correct\n"
          ]
        }
      ],
      "source": [
        "# You can use this cell to check if the output shape of attention'\n",
        "for dim in [512,768,1096]:\n",
        "  test_tensor = torch.rand(2,196,dim)\n",
        "  att_layer = Attention(dim,8,64,0.4)\n",
        "  output_tensor = att_layer(test_tensor)\n",
        "  equal =  test_tensor.shape == output_tensor.shape\n",
        "  if equal:\n",
        "    print('Good! For input dim: %d, the output shape is correct' %(dim))\n",
        "  else:\n",
        "    print('Uh-oh! For input dim: %d, the output shape is wrong' %(dim))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRGAesmOfk0A"
      },
      "source": [
        "The norm layer in Vision Transformer (ViT) is a layer that performs layer normalization on the input. It is typically applied after the Multi-Head Attention (MHA) and the MLP layers in the ViT architecture. The norm layer is used to help the model learn better representations by ensuring that the activations are normalized and centered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFbmcWQCF1rL"
      },
      "outputs": [],
      "source": [
        "### PreNorm function\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        # keey the residual connection here\n",
        "        return self.fn(self.norm(x), **kwargs)+x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l217s0ofF1rL"
      },
      "outputs": [],
      "source": [
        "#You can use\n",
        "a = PreNorm(768, Attention(768, heads = 8, dim_head = 64, dropout = 0.2))\n",
        "# to create a combination of layer norm and any other layer\n",
        "test_tensor = torch.rand(2,196,768)\n",
        "# you can use the following line to do the forward pass\n",
        "output_tensor = a(test_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De7Z8ywcF1rM"
      },
      "source": [
        "###Q1.3 PositionwiseFeedForward\n",
        "You will need to implement the posiotionwiseFeedForward layer in Vision Transformer.\n",
        "\n",
        "The FFN layer is called \"position-wise\" because it applies a separate feedforward network to each position in the sequence independently. It consists of two linear transformations with a non-linear activation function in between, typically GELU. The first linear transformation maps the input feature vector from its original dimension to a higher-dimensional space, and the second linear transformation maps it back to the original dimension. The output of the FFN layer is the element-wise sum of the input and the transformed feature vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP9qXq3AF1rM"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, dim, mlp_dim, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        \"\"\"\n",
        "         Args:\n",
        "          dim (int): input and output dimension.\n",
        "          mlp_dim (int): the output dimension of the first layer.\n",
        "          dropout (float): dropout rate for both linear layers.\n",
        "\n",
        "        Initialize an MLP.\n",
        "        You can use Pytorch's built-in nn.Linear function.\n",
        "        Input and output sizes of each layer:\n",
        "          1) fc1: dim, mlp_dim\n",
        "          2) fc2: mlp_dim, dim\n",
        "        \"\"\"\n",
        "\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        self.dropout = None\n",
        "        self.activation = nn.GELU()\n",
        "        ################# Your Implementations #################################\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor in the shape of (batch_size,N,dim).\n",
        "        Returns:\n",
        "            out (torch.Tensor): output tensor in the shape of (batch_size,N,dim).\n",
        "\n",
        "        The input tensor 'x' should pass through the following layers:\n",
        "        1) fc1: (batch_size,N,dim) ->  (batch_size,N,mlp_dim)\n",
        "        2) Apply activation function\n",
        "        3) Apply dropout\n",
        "        3) fc2: (batch_size,N,mlp_dim) -> (batch_size,N,dim)\n",
        "        4) Apply dropout\n",
        "        '''\n",
        "\n",
        "        out = None\n",
        "        ################# Your Implementations #################################\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o1D-3EsF1rM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c645aab-2124-46fd-a1a1-deb2c5e15949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good! For input dim: 512, the output shape is correct\n",
            "Good! For input dim: 768, the output shape is correct\n",
            "Good! For input dim: 1096, the output shape is correct\n"
          ]
        }
      ],
      "source": [
        "# You can use this cell to check if the output shape of PositionwiseFeedForward\n",
        "for dim in [512,768,1096]:\n",
        "  test_tensor = torch.rand(2,196,dim)\n",
        "  ffn = PositionwiseFeedForward(dim,dim*4,0.1)\n",
        "  output_tensor = ffn(test_tensor)\n",
        "  equal =  test_tensor.shape == output_tensor.shape\n",
        "  if equal:\n",
        "    print('Good! For input dim: %d, the output shape is correct' %(dim))\n",
        "  else:\n",
        "    print('Uh-oh! For input dim: %d, the output shape is wrong' %(dim))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoRkYfdoF1rM"
      },
      "source": [
        "### Q1.4 TransformerBlock\n",
        "Now you can follow the steps and use above class to implement the standard transformer block as demostrated in the following image.\n",
        "\n",
        " <img src=\"https://web.cs.ucla.edu/~smo3/cs188/assignment3/transformer_block.png\"  width=\"20%\" height=\"40%\">\n",
        "\n",
        "1. Apply Layer-norm to the input tensor\n",
        "2. Apply the Multi-Head Attention (MHA) layer to the output tensor from step1. The MHA layer takes in the input tensor, and returns the attention scores and the attention output tensor.\n",
        "3. Add the residual connection to the output of the MHA layer.\n",
        "4. Apply Layer-norm to output of last step\n",
        "5. Apply the Position-wise Feedforward Network (FFN) layer to the output of the previous step. The FFN layer takes in the output tensor, and returns the transformed output tensor.\n",
        "6. Add the residual connection to the output of the FFN layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMCLL9IJF1rN"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        \"Implements Transformer block.\"\n",
        "        super().__init__()\n",
        "        '''\n",
        "        Args:\n",
        "          dim (int): input and output dimension.\n",
        "          heads (int): number of attention heads.\n",
        "          dim_head (int): input dimension of each attention head.\n",
        "          mlp_dim (int):\n",
        "          dropout (float): dropout rate for attention and FFN layers.\n",
        "\n",
        "        '''\n",
        "        # Use the PreNorm,Attention and PositionwiseFeedForword class to build your\n",
        "        # Transformer block\n",
        "        self.attn = None\n",
        "        self.ff = None\n",
        "\n",
        "        ################# Your Implementations #################################\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor in the shape of (batch_size,N,dim).\n",
        "        Returns:\n",
        "            out (torch.Tensor): output tensor in the shape of (batch_size,N,dim).\n",
        "        \"\"\"\n",
        "        ################# Your Implementations #################################\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLAFJkurF1rN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8b787e2-bdd0-4bfd-c0b7-2ad2226f2141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good! For input dim: 512, the output shape is correct\n",
            "Good! For input dim: 768, the output shape is correct\n",
            "Good! For input dim: 1096, the output shape is correct\n"
          ]
        }
      ],
      "source": [
        "# You can use this cell to check if the output shape of Transformer\n",
        "for dim in [512,768,1096]:\n",
        "  test_tensor = torch.rand(2,196,dim)\n",
        "  transformer_block = Transformer(dim,8,64,dim*4,0.1)\n",
        "  output_tensor = transformer_block(test_tensor)\n",
        "  equal =  test_tensor.shape == output_tensor.shape\n",
        "  if equal:\n",
        "    print('Good! For input dim: %d, the output shape is correct' %(dim))\n",
        "  else:\n",
        "    print('Uh-oh! For input dim: %d, the output shape is wrong' %(dim))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYrPbuC3F1rN"
      },
      "source": [
        "###Q1.5 ViTModel\n",
        "Now you can use above classes to build your Vision Transfromer. Recall the ViT Architecture.\n",
        "\n",
        " <img src=\"https://web.cs.ucla.edu/~smo3/cs188/assignment3/vit.png\"  width=\"40%\" height=\"40%\">\n",
        "\n",
        " Recall the pipline for Vision Transformer model:\n",
        "\n",
        "1. Load the input images and preprocess them into a set of image patches. The patches should be non-overlapping and should cover the entire input image. Each patch should be flattened into a vector and projected into a lower-dimensional/equal-dimensional space using a linear layer.\n",
        "\n",
        "2. Add cls token and learnable positional embeddings to the projected patch vectors. The positional embedding should encode the spatial location of each patch in the input image.\n",
        "\n",
        "3. Stack several Transformer blocks to process the patch vectors. Each Transformer block should consist of a Multi-Head Attention (MHA) layer and a Position-wise Feedforward Network (FFN) layer, with residual connections and layer normalization applied after each layer.\n",
        "\n",
        "3. Apply a mean pooling operation over the output of the last Transformer block or take the output vector related to the cls token to obtain a fixed-size feature vector.\n",
        "\n",
        "5. Feed the feature vector into a fully-connected classification head to predict the class label of the input image.\n",
        "\n",
        "6. Train the model using a supervised learning objective, such as cross-entropy loss, and backpropagation to update the model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h8oMh2vF1rN"
      },
      "outputs": [],
      "source": [
        "# helper method\n",
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"Implements Vision Transfromer\"\n",
        "    def __init__(self, *,\n",
        "                 image_size,\n",
        "                 patch_size,\n",
        "                 num_classes,\n",
        "                 dim,\n",
        "                 depth,\n",
        "                 heads,\n",
        "                 mlp_dim,\n",
        "                 pool = 'cls',\n",
        "                 channels = 3,\n",
        "                 dim_head = 64,\n",
        "                 dropout = 0.,\n",
        "                 emb_dropout = 0.,\n",
        "                ):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          image_size (int): the height/weight of the input image.\n",
        "          patch_size (int): image patch size. In the ViT paper, this value is 16.\n",
        "          num_classes (num_class): Number of image classes for MLP prediction head.\n",
        "          dim (int): patch and position embedding dimension.\n",
        "          depth (int): number of stacked transformer blocks.\n",
        "          heads (int): number of attention heads.\n",
        "          mlp_dim (int): inner dimension for MLP in transformer blocks.\n",
        "          pool (str): choice between \"cls\", \"mean\", \"none\".\n",
        "                      For cls, you will need to use the cls token for perdiction\n",
        "                      For mean, you will need to take the mean of last transformer output\n",
        "                      For none, you can just return the last transformer output.\n",
        "                                This will mainly be used for dense prediction tasks.\n",
        "          channels (int): Input image channels. Set to 3 for RGB image.\n",
        "          dropout (float): dropout rate for transformer blocks.\n",
        "          emb_dropout (float): dropout rate for patch embedding.\n",
        "        \"\"\"\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = 0\n",
        "        patch_dim = 0\n",
        "\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Compute the num_patches and patch_dim\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "        assert pool in {'cls', 'mean', 'none'}, 'pool type must be either cls (cls token), mean (mean pooling), or none (no pooling)'\n",
        "        self.pool = pool\n",
        "\n",
        "        self.to_patch_embedding = None\n",
        "\n",
        "        self.pos_embedding = None\n",
        "        self.cls_token = None\n",
        "        self.dropout = None\n",
        "        self.transformers = nn.ModuleList([])\n",
        "        self.mlp_head = None\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO:\n",
        "        # 1) Define self.to_patch_embedding usinng the Tokenization_layer class\n",
        "        # 2) Define learnable 1-D pos_embedding using torch.randn, the number of\n",
        "        #    embedding should be num_patches+1\n",
        "        # 3) Define learnable 1-D cls_token with dimension = dim. You can use\n",
        "        #    nn.Parameter and torch.randn to initialize this\n",
        "        # 4) Define dropout with emb_dropout\n",
        "        # 5) Define array of d Transformer modules, where d=depth\n",
        "        # 6) Using nn.Sqeuential to create the MLP head including two layers:\n",
        "        #    The first layer in the MLP head is a LayerNorm layer.\n",
        "        #    The second layer in the MLP head is a linear layer change dimension to num_classes\n",
        "        #    Note that this MLP head should have 'dim' input dimensions, not 'mlp_dim' which is\n",
        "        #    used for the MLP in the transformer block instead.\n",
        "\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        '''\n",
        "        Args:\n",
        "            img (torch.Tensor): input tensor in the shape of (batch_size,C,H,W).\n",
        "        Returns:\n",
        "            out (torch.Tensor): output tensor in the shape of (batch_size,num_class).\n",
        "\n",
        "        The input tensor 'img' should pass through the following layers:\n",
        "        1) self.to_patch_embedding: (batch_size,C,H,W) -> (batch_size,N,dim)\n",
        "        2) Using torch.Tensor.repeat to repeat the cls alone batch dimension.\n",
        "           Then, concatenate with cls token (batch_size,N,dim) -> (batch_size,N+1,dim)\n",
        "        3) Take sum of patch embedding and position embedding, then apply dropout.\n",
        "        4) Passing through all the transformer blocks (batch_size,N+1,dim) -> (batch_size,N+1,dim)\n",
        "        5) If pool is none, simply return the output of (4). Else, proceed to (5).\n",
        "        5) Use cls token or use pool method to get latent code of batched images\n",
        "            (batch_size,N+1,dim) -> (batch_size,dim)\n",
        "        6) Apply MLP head to the output of last step: (batch_size,dim) -> (batch_size,num_class)\n",
        "\n",
        "        '''\n",
        "        out = None\n",
        "        ################# Your Implementations #################################\n",
        "\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8oNroxoyG1r"
      },
      "source": [
        "Then let's train your ViT model with with cls token as pool policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbp_mTZJF1rO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ef4428-5418-47bc-d717-2c6bb838374f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 1563/1563 [02:02<00:00, 12.78it/s, loss=3.93]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 3.7836, Accuracy = 0.1214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 1563/1563 [02:02<00:00, 12.79it/s, loss=3.54]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 3.5456, Accuracy = 0.1539\n"
          ]
        }
      ],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "#Define the model, optimizer, and criterion (loss_fn)\n",
        "model = ViT(image_size = 128,\n",
        "    patch_size = 16,\n",
        "    num_classes = 100,\n",
        "    dim = 192,\n",
        "    depth = 8,\n",
        "    heads = 4,\n",
        "    dim_head = 48,\n",
        "    mlp_dim = 768,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        "           )\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=1e-4,)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Define the dataset and data transform with flatten functions appended\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "# Define the batch size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIF3rbTjxRqC"
      },
      "source": [
        "I got an accuracy of 14.43% using my own implementation. How about you?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGXNfFX9zEXb"
      },
      "source": [
        "Then let's train your ViT model with with average pooling as pool policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWLFLdbwy3ko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77302b9-cfe5-4ca3-dcbe-02c47420418b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 1563/1563 [02:14<00:00, 11.60it/s, loss=3.89]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 3.8176, Accuracy = 0.1161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 1563/1563 [02:03<00:00, 12.65it/s, loss=3.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 3.5833, Accuracy = 0.1525\n"
          ]
        }
      ],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "#Define the model, optimizer, and criterion (loss_fn)\n",
        "model = ViT(image_size = 128,\n",
        "    patch_size = 16,\n",
        "    num_classes = 100,\n",
        "    dim = 192,\n",
        "    depth = 8,\n",
        "    heads = 4,\n",
        "    pool = 'mean',\n",
        "    dim_head = 48,\n",
        "    mlp_dim = 768,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        "           )\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=1e-4,)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Define the dataset and data transform with flatten functions appended\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "# Define the batch size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIsqefHhZLUj"
      },
      "source": [
        "I got an accuracy of 14.11% using my own implementation. How about you?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "X42ej8ZhF1q4"
      ],
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}